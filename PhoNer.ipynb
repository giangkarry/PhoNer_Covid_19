{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PhoNer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX5tHMDsQKEx"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install tensorflow==1.15.0\n",
        "!pip install transformers scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!git clone https://github.com/VinAIResearch/PhoNER_COVID19\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0U8_Gb2bwBK6",
        "outputId": "87004d35-1baa-43ac-fd60-b8cce136a1d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'PhoNER_COVID19' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def read_data(file_path):\n",
        "    tokens = []\n",
        "    tags = []\n",
        "    \n",
        "    tokens_temp = []\n",
        "    tags_temp = []\n",
        "    for line in open(file_path, encoding='utf-8'):\n",
        "        line = line.strip()\n",
        "        if not line: \n",
        "            if tokens_temp:\n",
        "                tokens.append(tokens_temp)\n",
        "                tags.append(tags_temp)\n",
        "            tokens_temp = []\n",
        "            tags_temp = []\n",
        "        else:\n",
        "            token, tag = line.split()        \n",
        "            tokens_temp.append(token)\n",
        "            tags_temp.append(tag)\n",
        "            \n",
        "    return tokens, tags\n"
      ],
      "metadata": {
        "id": "G4Cn16wDkcY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_tokens, train_tags = read_data(\"PhoNER_COVID19/data/syllable/train_syllable.conll\")\n",
        "validation_tokens, validation_tags = read_data(\"PhoNER_COVID19/data/syllable/dev_syllable.conll\")\n",
        "test_tokens, test_tags = read_data(\"PhoNER_COVID19/data/syllable/test_syllable.conll\")\n"
      ],
      "metadata": {
        "id": "qv2NxlwzlUZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(len(train_tokens))\n",
        "print(len(validation_tokens))\n",
        "print(len(test_tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eKpgyDtzCZP",
        "outputId": "a5abc1ce-6088-443a-db0b-81f41427079a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5027\n",
            "2000\n",
            "3000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(1):\n",
        "    for token, tag in zip(train_tokens[i], train_tags[i]):\n",
        "        print('%s\\t%s' % (token, tag))\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EmLiqSYliMA",
        "outputId": "343c8022-0052-422f-bf97-9841df295739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Đồng\tO\n",
            "thời\tO\n",
            ",\tO\n",
            "bệnh\tO\n",
            "viện\tO\n",
            "tiếp\tO\n",
            "tục\tO\n",
            "thực\tO\n",
            "hiện\tO\n",
            "các\tO\n",
            "biện\tO\n",
            "pháp\tO\n",
            "phòng\tO\n",
            "chống\tO\n",
            "dịch\tO\n",
            "bệnh\tO\n",
            "COVID\tO\n",
            "-\tO\n",
            "19\tO\n",
            "theo\tO\n",
            "hướng\tO\n",
            "dẫn\tO\n",
            "của\tO\n",
            "Bộ\tB-ORGANIZATION\n",
            "Y\tI-ORGANIZATION\n",
            "tế\tI-ORGANIZATION\n",
            ".\tO\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "def build_dict(tokens_or_tags, special_tokens):\n",
        "    tok2idx = defaultdict(lambda: 0)\n",
        "    idx2tok = []\n",
        "    \n",
        "    vocab = set([t for ts in tokens_or_tags for t in ts])\n",
        "    voab_size = len(vocab)+len(special_tokens)\n",
        "    idx2tok = ['']*voab_size\n",
        "\n",
        "    for i,token in enumerate(special_tokens):\n",
        "        tok2idx[token] = i\n",
        "        idx2tok[i] = token\n",
        "    \n",
        "    for i, token in enumerate(vocab, len(special_tokens)):\n",
        "        tok2idx[token] = i\n",
        "        idx2tok[i] = token       \n",
        "    \n",
        "    return tok2idx, idx2tok\n"
      ],
      "metadata": {
        "id": "M8V_LGmYlmDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "special_tokens = ['', '']\n",
        "special_tags = ['O']\n",
        " \n",
        "token2idx, idx2token = build_dict(train_tokens + validation_tokens, special_tokens)\n",
        "tag2idx, idx2tag = build_dict(train_tags, special_tags)\n"
      ],
      "metadata": {
        "id": "gc3XblgYl7vZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tag2idx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrOWo7RLl_Eo",
        "outputId": "72b6df09-beca-4c13-a93d-b3834e52733b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(<function __main__.build_dict.<locals>.<lambda>>,\n",
              "            {'B-AGE': 4,\n",
              "             'B-DATE': 9,\n",
              "             'B-GENDER': 18,\n",
              "             'B-JOB': 7,\n",
              "             'B-LOCATION': 17,\n",
              "             'B-NAME': 10,\n",
              "             'B-ORGANIZATION': 11,\n",
              "             'B-PATIENT_ID': 12,\n",
              "             'B-SYMPTOM_AND_DISEASE': 3,\n",
              "             'B-TRANSPORTATION': 6,\n",
              "             'I-AGE': 20,\n",
              "             'I-DATE': 2,\n",
              "             'I-GENDER': 14,\n",
              "             'I-JOB': 15,\n",
              "             'I-LOCATION': 19,\n",
              "             'I-NAME': 21,\n",
              "             'I-ORGANIZATION': 16,\n",
              "             'I-PATIENT_ID': 5,\n",
              "             'I-SYMPTOM_AND_DISEASE': 1,\n",
              "             'I-TRANSPORTATION': 13,\n",
              "             'O': 8})"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def words2idxs(tokens_list):\n",
        "    return [token2idx[word] for word in tokens_list]\n",
        " \n",
        "def tags2idxs(tags_list):\n",
        "    return [tag2idx[tag] for tag in tags_list]\n",
        " \n",
        "def idxs2words(idxs):\n",
        "    return [idx2token[idx] for idx in idxs]\n",
        " \n",
        "def idxs2tags(idxs):\n",
        "    return [idx2tag[idx] for idx in idxs]\n"
      ],
      "metadata": {
        "id": "DElDG6WbmEcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def batches_generator(batch_size, tokens, tags, shuffle=True, allow_smaller_last_batch=True): \n",
        "    n_samples = len(tokens)\n",
        "    if shuffle:\n",
        "        order = np.random.permutation(n_samples)\n",
        "    else:\n",
        "        order = np.arange(n_samples)\n",
        " \n",
        "    n_batches = n_samples // batch_size\n",
        "    if allow_smaller_last_batch and n_samples % batch_size:\n",
        "        n_batches += 1\n",
        " \n",
        "    for k in range(n_batches):\n",
        "        batch_start = k * batch_size\n",
        "        batch_end = min((k + 1) * batch_size, n_samples)\n",
        "        current_batch_size = batch_end - batch_start\n",
        "        x_list = []\n",
        "        y_list = []\n",
        "        max_len_token = 0\n",
        "        for idx in order[batch_start: batch_end]:\n",
        "            x_list.append(words2idxs(tokens[idx]))\n",
        "            y_list.append(tags2idxs(tags[idx]))\n",
        "            max_len_token = max(max_len_token, len(tags[idx]))\n",
        " \n",
        "        x = np.ones([current_batch_size, max_len_token], dtype=np.int32) * token2idx['&amp;amp;amp;amp;lt;PAD&amp;amp;amp;amp;gt;']\n",
        "        y = np.ones([current_batch_size, max_len_token], dtype=np.int32) * tag2idx['O']\n",
        "        lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
        "        for n in range(current_batch_size):\n",
        "            utt_len = len(x_list[n])\n",
        "            x[n, :utt_len] = x_list[n]\n",
        "            lengths[n] = utt_len\n",
        "            y[n, :utt_len] = y_list[n]\n",
        "        yield x, y, lengths\n"
      ],
      "metadata": {
        "id": "VUXKFNpwmHe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
        "class BiLSTMModel():\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "r1A67hGbmK7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def declare_placeholders(self):\n",
        "    \"\"\"Specifies placeholders for the model.\"\"\"\n",
        " \n",
        "    # Placeholders for input and ground truth output.\n",
        "    self.input_batch = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None, None], name='input_batch')\n",
        "    self.ground_truth_tags = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None, None], name='ground_truth_tags')  \n",
        " \n",
        "    # Placeholder for lengths of the sequences.\n",
        "    self.lengths = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None], name='lengths') \n",
        " \n",
        "    # Placeholder for a dropout keep probability. If we don't feed\n",
        "    # a value for this placeholder, it will be equal to 1.0.\n",
        "    self.dropout_ph = tf.compat.v1.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
        " \n",
        "    # Placeholder for a learning rate (tf.float32).\n",
        "    self.learning_rate_ph = tf.compat.v1.placeholder(dtype=tf.float32, shape=[]) \n",
        " \n",
        "BiLSTMModel.__declare_placeholders = classmethod(declare_placeholders)\n"
      ],
      "metadata": {
        "id": "eXDYUiOamMyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_layers(self, vocabulary_size, embedding_dim, n_hidden_rnn, n_tags):\n",
        "   \"\"\"Specifies bi-LSTM architecture and computes logits for inputs.\"\"\"\n",
        " \n",
        "   # Create embedding variable (tf.Variable) with dtype tf.float32\n",
        "   initial_embedding_matrix = np.random.randn(vocabulary_size, embedding_dim) / np.sqrt(embedding_dim)\n",
        "   embedding_matrix_variable = tf.Variable(initial_value=initial_embedding_matrix, dtype=tf.float32) \n",
        " \n",
        "   # Create RNN cells (for example, tf.nn.rnn_cell.BasicLSTMCell) with n_hidden_rnn number of units\n",
        "   # and dropout (tf.nn.rnn_cell.DropoutWrapper), initializing all *_keep_prob with dropout placeholder.\n",
        "   forward_cell = tf.compat.v1.nn.rnn_cell.DropoutWrapper(tf.compat.v1.nn.rnn_cell.LSTMCell(n_hidden_rnn), self.dropout_ph, self.dropout_ph) \n",
        "   backward_cell = tf.compat.v1.nn.rnn_cell.DropoutWrapper(tf.compat.v1.nn.rnn_cell.LSTMCell(n_hidden_rnn), self.dropout_ph, self.dropout_ph)\n",
        " \n",
        "   # Look up embeddings for self.input_batch (tf.nn.embedding_lookup).\n",
        "   # Shape: [batch_size, sequence_len, embedding_dim].\n",
        "   embeddings = tf.nn.embedding_lookup(embedding_matrix_variable, self.input_batch) \n",
        " \n",
        "   # Pass them through Bidirectional Dynamic RNN (tf.nn.bidirectional_dynamic_rnn).\n",
        "   # Shape: [batch_size, sequence_len, 2 * n_hidden_rnn].\n",
        "   # Also don't forget to initialize sequence_length as self.lengths and dtype as tf.float32.\n",
        "   (rnn_output_fw, rnn_output_bw), _ = tf.compat.v1.nn.bidirectional_dynamic_rnn(forward_cell, backward_cell, embeddings, sequence_length=self.lengths, dtype=tf.float32) \n",
        "   rnn_output = tf.concat([rnn_output_fw, rnn_output_bw], axis=2)\n",
        " \n",
        "   # Dense layer on top.\n",
        "   # Shape: [batch_size, sequence_len, n_tags].\n",
        "   self.logits = tf.compat.v1.layers.dense(rnn_output, n_tags, activation=None)\n",
        " \n",
        "BiLSTMModel.__build_layers = classmethod(build_layers)\n"
      ],
      "metadata": {
        "id": "IQWHPkYhmPKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def compute_predictions(self):\n",
        "    \"\"\"Transforms logits to probabilities and finds the most probable tags.\"\"\"\n",
        " \n",
        "    # Create softmax (tf.nn.softmax) function\n",
        "    self.softmax_output = tf.nn.softmax(self.logits)\n",
        " \n",
        "    # Use argmax (tf.argmax) to get the most probable tags\n",
        "    self.predictions = tf.argmax(self.softmax_output, axis=-1)\n",
        "\n",
        "BiLSTMModel.__compute_predictions = classmethod(compute_predictions)\n"
      ],
      "metadata": {
        "id": "W8JviYy3mR-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def compute_loss(self, n_tags, PAD_index):\n",
        "    \"\"\"Computes masked cross-entopy loss with logits.\"\"\"\n",
        " \n",
        "    # Create cross entropy function function (tf.nn.softmax_cross_entropy_with_logits_v2)\n",
        "    ground_truth_tags_one_hot = tf.one_hot(self.ground_truth_tags, n_tags)\n",
        "    loss_tensor = tf.nn.softmax_cross_entropy_with_logits_v2(ground_truth_tags_one_hot, self.logits) \n",
        " \n",
        "    mask = tf.cast(tf.not_equal(self.input_batch, PAD_index), tf.float32)\n",
        "    # Create loss function which doesn't operate with &amp;amp;lt;PAD&amp;amp;gt; tokens (tf.reduce_mean)\n",
        "    # The argument of tf.reduce_mean should be multiplication of mask and loss_tensor.\n",
        "    self.loss =  tf.reduce_mean(mask*loss_tensor) \n",
        " \n",
        "BiLSTMModel.__compute_loss = classmethod(compute_loss)\n",
        "\n"
      ],
      "metadata": {
        "id": "MwfKHtcNmWI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def perform_optimization(self):\n",
        "    \"\"\"Specifies the optimizer and train_op for the model.\"\"\"\n",
        " \n",
        "    # Create an optimizer (tf.train.AdamOptimizer)\n",
        "    self.optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=self.learning_rate_ph)\n",
        "    self.grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
        " \n",
        "    # Gradient clipping (tf.clip_by_norm) for self.grads_and_vars\n",
        "    # Apply this operation only for gradients because self.grads_and_vars also contains variables.\n",
        "    # list comprehension might be useful in this case.\n",
        "    clip_norm = tf.cast(1.0, tf.float32)\n",
        "    self.grads_and_vars = [(tf.clip_by_norm(g, clip_norm), v) for (g,v) in self.grads_and_vars]\n",
        " \n",
        "    self.train_op = self.optimizer.apply_gradients(self.grads_and_vars)\n",
        " \n",
        "BiLSTMModel.__perform_optimization = classmethod(perform_optimization)\n",
        "\n"
      ],
      "metadata": {
        "id": "KTA35x8nmYIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def init_model(self, vocabulary_size, n_tags, embedding_dim, n_hidden_rnn, PAD_index):\n",
        "    self.__declare_placeholders()\n",
        "    self.__build_layers(vocabulary_size, embedding_dim, n_hidden_rnn, n_tags)\n",
        "    self.__compute_predictions()\n",
        "    self.__compute_loss(n_tags, PAD_index)\n",
        "    self.__perform_optimization()\n"
      ],
      "metadata": {
        "id": "N98SrsEkmaGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "BiLSTMModel.__init__ = classmethod(init_model)\n"
      ],
      "metadata": {
        "id": "vDSTKzuJmcV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_on_batch(self, session, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability):\n",
        "    feed_dict = {self.input_batch: x_batch,\n",
        "                 self.ground_truth_tags: y_batch,\n",
        "                 self.learning_rate_ph: learning_rate,\n",
        "                 self.dropout_ph: dropout_keep_probability,\n",
        "                 self.lengths: lengths}\n",
        " \n",
        "    session.run(self.train_op, feed_dict=feed_dict)\n",
        " \n",
        "BiLSTMModel.train_on_batch = classmethod(train_on_batch)\n"
      ],
      "metadata": {
        "id": "5nc0ODFBmeT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def predict_for_batch(self, session, x_batch, lengths):\n",
        " \n",
        "    feed_dict = {self.input_batch: x_batch,\n",
        "                 self.lengths: lengths}\n",
        " \n",
        "    predictions = session.run(self.predictions, feed_dict=feed_dict)\n",
        "    softmax_output = session.run(self.softmax_output, feed_dict=feed_dict)\n",
        " \n",
        "    return predictions, softmax_output\n",
        " \n",
        "BiLSTMModel.predict_for_batch = classmethod(predict_for_batch)\n"
      ],
      "metadata": {
        "id": "DoyWQw_Wmhg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from evaluation import precision_recall_f1\n",
        "\n",
        "def predict_tags(model, session, token_idxs_batch, lengths):\n",
        "    \"\"\"Performs predictions and transforms indices to tokens and tags.\"\"\"\n",
        " \n",
        "    tag_idxs_batch, softmax_batch = model.predict_for_batch(session, token_idxs_batch, lengths)\n",
        " \n",
        "    tags_batch, tokens_batch, probs_batch = [], [], []\n",
        "    for tag_idxs, token_idxs, softmax_probs in zip(tag_idxs_batch, token_idxs_batch, softmax_batch):\n",
        "        tags, tokens, probs = [], [], []\n",
        "        for tag_idx, token_idx, softmax_prob in zip(tag_idxs, token_idxs, softmax_probs):\n",
        "            tags.append(idx2tag[tag_idx])\n",
        "            tokens.append(idx2token[token_idx])\n",
        "            probs.append(softmax_prob)\n",
        "        tags_batch.append(tags)\n",
        "        tokens_batch.append(tokens)\n",
        "        probs_batch.append(probs)\n",
        "    return tags_batch, tokens_batch, probs_batch\n",
        " \n",
        "def eval_conll(model, session, tokens, tags, short_report=True):\n",
        "    \"\"\"Computes NER quality measures using CONLL shared task script.\"\"\"\n",
        " \n",
        "    y_true, y_pred = [], []\n",
        "    for x_batch, y_batch, lengths in batches_generator(1, tokens, tags):\n",
        "        tags_batch, tokens_batch, probs_batch = predict_tags(model, session, x_batch, lengths)\n",
        "        if len(x_batch[0]) != len(tags_batch[0]):\n",
        "            raise Exception(\"Incorrect length of prediction for the input, \"\n",
        "                            \"expected length: %i, got: %i\" % (len(x_batch[0]), len(tags_batch[0])))\n",
        "        predicted_tags = []\n",
        "        ground_truth_tags = []\n",
        "        for gt_tag_idx, pred_tag, token in zip(y_batch[0], tags_batch[0], tokens_batch[0]):\n",
        "            if token != '&amp;amp;lt;PAD&amp;amp;gt;':\n",
        "                ground_truth_tags.append(idx2tag[gt_tag_idx])\n",
        "                predicted_tags.append(pred_tag)\n",
        " \n",
        "        # We extend every prediction and ground truth sequence with 'O' tag\n",
        "        # to indicate a possible end of entity.\n",
        "        y_true.extend(ground_truth_tags + ['O'])\n",
        "        y_pred.extend(predicted_tags + ['O'])\n",
        " \n",
        "    results = precision_recall_f1(y_true, y_pred, print_results=True, short_report=short_report)\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "CQZrCtiumjbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "model = BiLSTMModel(vocabulary_size=len(token2idx), n_tags=len(tag2idx), embedding_dim=200, n_hidden_rnn=200, PAD_index=token2idx['<PAD>']) \n",
        "\n",
        "batch_size = 32\n",
        "n_epochs = 4 \n",
        "learning_rate = 0.005\n",
        "learning_rate_decay = np.sqrt(2) \n",
        "dropout_keep_probability = 0.5\n"
      ],
      "metadata": {
        "id": "iiz4vR97mlhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sess = tf.compat.v1.Session()\n",
        "sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "print('Start training... \\n')\n",
        "for epoch in range(n_epochs):\n",
        "    # For each epoch evaluate the model on train and validation data\n",
        "    print('-' * 20 + ' Epoch {} '.format(epoch+1) + 'of {} '.format(n_epochs) + '-' * 20)\n",
        "    print('Train data evaluation:')\n",
        "    eval_conll(model, sess, train_tokens, train_tags, short_report=True)\n",
        "    print('Validation data evaluation:')\n",
        "    eval_conll(model, sess, validation_tokens, validation_tags, short_report=True)\n",
        "    \n",
        "    # Train the model\n",
        "    for x_batch, y_batch, lengths in batches_generator(batch_size, train_tokens, train_tags):\n",
        "        model.train_on_batch(sess, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability)\n",
        "        \n",
        "    # Decaying the learning rate\n",
        "    learning_rate = learning_rate / learning_rate_decay\n",
        "    \n",
        "print('...training finished.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQwGO_8mnB6V",
        "outputId": "efe72504-5a11-43cd-828f-fa21cc935175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training... \n",
            "\n",
            "-------------------- Epoch 1 of 4 --------------------\n",
            "Train data evaluation:\n",
            "processed 172568 tokens with 15767 phrases; found: 113308 phrases; correct: 592.\n",
            "\n",
            "precision:  0.52%; recall:  3.75%; F1:  0.92\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 73325 tokens with 7478 phrases; found: 48628 phrases; correct: 270.\n",
            "\n",
            "precision:  0.56%; recall:  3.61%; F1:  0.96\n",
            "\n",
            "-------------------- Epoch 2 of 4 --------------------\n",
            "Train data evaluation:\n",
            "processed 172568 tokens with 15767 phrases; found: 17528 phrases; correct: 14383.\n",
            "\n",
            "precision:  82.06%; recall:  91.22%; F1:  86.40\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 73325 tokens with 7478 phrases; found: 8311 phrases; correct: 6383.\n",
            "\n",
            "precision:  76.80%; recall:  85.36%; F1:  80.85\n",
            "\n",
            "-------------------- Epoch 3 of 4 --------------------\n",
            "Train data evaluation:\n",
            "processed 172568 tokens with 15767 phrases; found: 15757 phrases; correct: 14513.\n",
            "\n",
            "precision:  92.11%; recall:  92.05%; F1:  92.08\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 73325 tokens with 7478 phrases; found: 7352 phrases; correct: 6220.\n",
            "\n",
            "precision:  84.60%; recall:  83.18%; F1:  83.88\n",
            "\n",
            "-------------------- Epoch 4 of 4 --------------------\n",
            "Train data evaluation:\n",
            "processed 172568 tokens with 15767 phrases; found: 16011 phrases; correct: 15086.\n",
            "\n",
            "precision:  94.22%; recall:  95.68%; F1:  94.95\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 73325 tokens with 7478 phrases; found: 7517 phrases; correct: 6526.\n",
            "\n",
            "precision:  86.82%; recall:  87.27%; F1:  87.04\n",
            "\n",
            "...training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('-' * 20 + ' Train set quality: ' + '-' * 20)\n",
        "train_results = eval_conll(model, sess, train_tokens, train_tags, short_report=False)\n",
        "\n",
        "print('-' * 20 + ' Validation set quality: ' + '-' * 20)\n",
        "validation_results = eval_conll(model, sess, validation_tokens, validation_tags, short_report=False) \n",
        "\n",
        "print('-' * 20 + ' Test set quality: ' + '-' * 20)\n",
        "test_results = eval_conll(model, sess, test_tokens, test_tags, short_report=False) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_-hTT7AueEX",
        "outputId": "2f87c7ac-86ae-499f-a8db-270cbae2e585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------- Train set quality: --------------------\n",
            "processed 172568 tokens with 15767 phrases; found: 15941 phrases; correct: 15235.\n",
            "\n",
            "precision:  95.57%; recall:  96.63%; F1:  96.10\n",
            "\n",
            "\t         AGE: precision:   96.19%; recall:   96.19%; F1:   96.19; predicted:   682\n",
            "\n",
            "\t        DATE: precision:   99.02%; recall:   99.33%; F1:   99.18; predicted:  2557\n",
            "\n",
            "\t      GENDER: precision:   93.85%; recall:   90.04%; F1:   91.90; predicted:   520\n",
            "\n",
            "\t         JOB: precision:   69.39%; recall:   66.34%; F1:   67.83; predicted:   196\n",
            "\n",
            "\t    LOCATION: precision:   96.09%; recall:   97.96%; F1:   97.02; predicted:  5503\n",
            "\n",
            "\t        NAME: precision:   83.43%; recall:   80.80%; F1:   82.10; predicted:   338\n",
            "\n",
            "\tORGANIZATION: precision:   91.15%; recall:   93.32%; F1:   92.22; predicted:  1164\n",
            "\n",
            "\t  PATIENT_ID: precision:   98.59%; recall:   99.35%; F1:   98.97; predicted:  3265\n",
            "\n",
            "\tSYMPTOM_AND_DISEASE: precision:   90.74%; recall:   94.02%; F1:   92.35; predicted:  1491\n",
            "\n",
            "\tTRANSPORTATION: precision:   97.78%; recall:   97.35%; F1:   97.56; predicted:   225\n",
            "\n",
            "-------------------- Validation set quality: --------------------\n",
            "processed 73325 tokens with 7478 phrases; found: 7463 phrases; correct: 6605.\n",
            "\n",
            "precision:  88.50%; recall:  88.33%; F1:  88.41\n",
            "\n",
            "\t         AGE: precision:   93.48%; recall:   95.29%; F1:   94.38; predicted:   368\n",
            "\n",
            "\t        DATE: precision:   96.34%; recall:   95.56%; F1:   95.95; predicted:  1094\n",
            "\n",
            "\t      GENDER: precision:   93.31%; recall:   90.61%; F1:   91.94; predicted:   269\n",
            "\n",
            "\t         JOB: precision:   55.88%; recall:   43.18%; F1:   48.72; predicted:   102\n",
            "\n",
            "\t    LOCATION: precision:   87.77%; recall:   90.50%; F1:   89.12; predicted:  2822\n",
            "\n",
            "\t        NAME: precision:   74.02%; recall:   50.00%; F1:   59.68; predicted:   127\n",
            "\n",
            "\tORGANIZATION: precision:   76.55%; recall:   78.22%; F1:   77.38; predicted:   563\n",
            "\n",
            "\t  PATIENT_ID: precision:   95.89%; recall:   94.98%; F1:   95.43; predicted:  1264\n",
            "\n",
            "\tSYMPTOM_AND_DISEASE: precision:   78.91%; recall:   79.63%; F1:   79.27; predicted:   773\n",
            "\n",
            "\tTRANSPORTATION: precision:   92.59%; recall:   86.21%; F1:   89.29; predicted:    81\n",
            "\n",
            "-------------------- Test set quality: --------------------\n",
            "processed 111354 tokens with 11735 phrases; found: 11585 phrases; correct: 10196.\n",
            "\n",
            "precision:  88.01%; recall:  86.89%; F1:  87.44\n",
            "\n",
            "\t         AGE: precision:   93.63%; recall:   93.47%; F1:   93.55; predicted:   581\n",
            "\n",
            "\t        DATE: precision:   96.13%; recall:   96.07%; F1:   96.10; predicted:  1653\n",
            "\n",
            "\t      GENDER: precision:   95.93%; recall:   91.77%; F1:   93.81; predicted:   442\n",
            "\n",
            "\t         JOB: precision:   55.75%; recall:   36.42%; F1:   44.06; predicted:   113\n",
            "\n",
            "\t    LOCATION: precision:   86.50%; recall:   88.56%; F1:   87.52; predicted:  4547\n",
            "\n",
            "\t        NAME: precision:   78.57%; recall:   48.43%; F1:   59.92; predicted:   196\n",
            "\n",
            "\tORGANIZATION: precision:   71.57%; recall:   74.45%; F1:   72.98; predicted:   802\n",
            "\n",
            "\t  PATIENT_ID: precision:   95.28%; recall:   95.66%; F1:   95.47; predicted:  2013\n",
            "\n",
            "\tSYMPTOM_AND_DISEASE: precision:   79.10%; recall:   74.65%; F1:   76.81; predicted:  1072\n",
            "\n",
            "\tTRANSPORTATION: precision:   89.76%; recall:   77.20%; F1:   83.01; predicted:   166\n",
            "\n"
          ]
        }
      ]
    }
  ]
}